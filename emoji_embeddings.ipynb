{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GDN_0q27Rr0f"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os.path as path\n",
    "import numpy as np\n",
    "import nltk.tokenize as tk\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "import sklearn.cluster\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aEk1wsgJKPrS"
   },
   "outputs": [],
   "source": [
    "def get_all_examples():\n",
    "    \"\"\"Load all examples from a CSV file using pandas and return those that contain text.\n",
    "\n",
    "    Returns:\n",
    "        A list of tweets (or data) that contain text.\n",
    "    \"\"\"\n",
    "    save_file = './twitter_emoji.csv'\n",
    "\n",
    "    if path.exists(save_file):\n",
    "        df = pd.read_csv(save_file, lineterminator = '\\n')\n",
    "        ids_to_examples = df[df['Text'].notna()].to_dict(orient='records')  # Convert to list of dicts\n",
    "    else:\n",
    "        print('Could not find tweets, so returning an empty list!')\n",
    "        ids_to_examples = []\n",
    "\n",
    "    return ids_to_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eCcC1etXimzn"
   },
   "outputs": [],
   "source": [
    "def get_tweets(raw_examples):\n",
    "  tweets = []\n",
    "  for tweet in raw_examples:\n",
    "    for (key, value) in tweet.items():\n",
    "      tweets.append(value)\n",
    "  return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHVUxivsR26L",
    "outputId": "ba4ea6de-6157-4454-d608-4aee9a8e842d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üß°', '@KeplerHomes', 'AirdropBox', 'event', 'for', '#Arbitrum', 'ecological', 'users', 'is', 'here.', 'A', 'total', 'of', '550,000', 'addresses', 'are', 'eligible', 'for', '#airdrop,', 'and', '5', 'types', 'of', 'AirDropbox', 'with', 'different', 'scarcity', 'can', 'be', 'issued.', '‚ù§Ô∏è', 'üíô', 'Invitation', 'code:', '52DC39', 'üèÜ', 'Airdrop', 'Portal:', 'üëâ', 'https://t.co/fudohu97uV']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Input tweet\n",
    "tweet = \"\"\"üß°@KeplerHomes AirdropBox event for #Arbitrum ecological users is here. A total of 550,000 addresses are eligible for #airdrop, and 5 types of AirDropbox with different scarcity can be issued.\n",
    "‚ù§Ô∏è\n",
    "üíôInvitation code: 52DC39\n",
    "üèÜAirdrop Portal:üëâ https://t.co/fudohu97uV\"\"\"\n",
    "\n",
    "# gets rid of punctuation and https\n",
    "def remove_gack(words):\n",
    "  punctation = \".,/';:[]\\-=`~!@#$%^&*()_+{}|<>?\"\n",
    "  for punc in punctation:\n",
    "    while punc in words:\n",
    "      words.remove(punc)\n",
    "  while \"https\" in words:\n",
    "    words.remove(\"https\")\n",
    "  return words\n",
    "\n",
    "\n",
    "# Convert tweet into an array of words, keeping emojis\n",
    "def tweet_to_words_with_emojis(_tweet):\n",
    "    tweet = _tweet\n",
    "    # Match words, hashtags, mentions, URLs, numbers, and emojis\n",
    "    # words = re.findall(r'\\w+|#\\w+|@\\w+|https?://\\S+|[^\\w\\s]', tweet)\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for em in emoji.emoji_list(tweet):\n",
    "        em = em[\"emoji\"]\n",
    "        tweet = tweet.replace(em, \" \" + em + \" \")\n",
    "    while \"  \" in tweet:\n",
    "      tweet = tweet.replace(\"  \", \" \")\n",
    "    tweet = tweet.replace(punc, \"\").split()\n",
    "    \n",
    "    # words = remove_gack(words)\n",
    "    return tweet\n",
    "\n",
    "# Process the tweet\n",
    "words_array = tweet_to_words_with_emojis(tweet)\n",
    "\n",
    "# Print the result\n",
    "print(words_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = []\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/backhand_index_pointing_right.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/check_mark_button.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/check_mark.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/clown_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/cooking.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/egg.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/enraged_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/eyes.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/face_holding_back_tears.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/face_savoring_food.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/face_with_steam_from_nose.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/face_with_tears_of_joy.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/fearful_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/fire.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/folded_hands.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/ghost.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/grinning_face_with_sweat.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/hatching_chick.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/hot_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/loudly_crying_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/melting_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/middle_finger.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/party_popper.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/partying_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/pile_of_poo.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/rabbit_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/rabbit.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/red_heart.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/rolling_on_the_floor_laughing.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/saluting_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/skull.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face_with_halo.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face_with_heart-eyes.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face_with_hearts.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face_with_sunglasses.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face_with_tear.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/smiling_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/sparkles.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/sun.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/thinking_face.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/thumbs_up.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/white_heart.csv\", lineterminator='\\n')[\"Text\"]))\n",
    "all_tweets.extend(list(pd.read_csv(\"emoji_twitter_dataset2/winking_face.csv\", lineterminator='\\n')[\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsV-uU0kdUg0",
    "outputId": "a790f77c-3f03-4cf6-99b3-f1131b3815e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['üß°',\n",
       "  '@KeplerHomes',\n",
       "  'AirdropBox',\n",
       "  'event',\n",
       "  'for',\n",
       "  '#Arbitrum',\n",
       "  'ecological',\n",
       "  'users',\n",
       "  'is',\n",
       "  'here.',\n",
       "  'A',\n",
       "  'total',\n",
       "  'of',\n",
       "  '550,000',\n",
       "  'addresses',\n",
       "  'are',\n",
       "  'eligible',\n",
       "  'for',\n",
       "  '#airdrop,',\n",
       "  'and',\n",
       "  '5',\n",
       "  'types',\n",
       "  'of',\n",
       "  'AirDropbox',\n",
       "  'with',\n",
       "  'different',\n",
       "  'scarcity',\n",
       "  'can',\n",
       "  'be',\n",
       "  'issued.',\n",
       "  'üíô',\n",
       "  'Invitation',\n",
       "  'code:',\n",
       "  '52DC39',\n",
       "  'üèÜ',\n",
       "  'Airdrop',\n",
       "  'Portal:',\n",
       "  'üëâ',\n",
       "  'https://t.co/fudohu97uV'],\n",
       " ['Remember,',\n",
       "  'success',\n",
       "  'in',\n",
       "  'online',\n",
       "  'business',\n",
       "  'is',\n",
       "  'a',\n",
       "  'marathon,',\n",
       "  'not',\n",
       "  'a',\n",
       "  'sprint.',\n",
       "  'Keep',\n",
       "  'at',\n",
       "  'it,',\n",
       "  'stay',\n",
       "  'focused,',\n",
       "  'and',\n",
       "  'success',\n",
       "  'will',\n",
       "  'come.\"',\n",
       "  '#patience',\n",
       "  '#onlinebusiness',\n",
       "  '#success',\n",
       "  'For',\n",
       "  'more',\n",
       "  'tips',\n",
       "  'and',\n",
       "  'Strategies,',\n",
       "  'follow',\n",
       "  'me',\n",
       "  'üëâ',\n",
       "  '@coach_lawrence1',\n",
       "  'https://t.co/IvtL9Om86J'],\n",
       " ['@occupied_9',\n",
       "  '@Rhiannon_clare_',\n",
       "  '@FightHaven',\n",
       "  'Thanks',\n",
       "  'for',\n",
       "  'the',\n",
       "  'update',\n",
       "  'the',\n",
       "  'sh*t',\n",
       "  'country',\n",
       "  'the',\n",
       "  'sh*t',\n",
       "  'police',\n",
       "  'üëâ',\n",
       "  'üí©',\n",
       "  'üò°'],\n",
       " ['Hungry',\n",
       "  'for',\n",
       "  'active',\n",
       "  'mutuals',\n",
       "  'üë¨',\n",
       "  'üë≠',\n",
       "  '?',\n",
       "  'Follow',\n",
       "  'fastest',\n",
       "  'Retweeters',\n",
       "  'üîÅ',\n",
       "  'now',\n",
       "  'üíØ',\n",
       "  '‚úÖ',\n",
       "  'https://t.co/kkiD7of60J'],\n",
       " ['üì¢',\n",
       "  \"It's\",\n",
       "  'confirmed.',\n",
       "  'Whitelist',\n",
       "  'for',\n",
       "  'Shardeum',\n",
       "  'Airdrop',\n",
       "  'is',\n",
       "  'here.',\n",
       "  'You',\n",
       "  'can',\n",
       "  'now',\n",
       "  'join',\n",
       "  'our',\n",
       "  'whitelist,',\n",
       "  'there',\n",
       "  'are',\n",
       "  'over',\n",
       "  '30000',\n",
       "  'spots.',\n",
       "  '(First',\n",
       "  'come',\n",
       "  'First',\n",
       "  'Served)',\n",
       "  'üëâ',\n",
       "  'https://t.co/aHulc12YTO',\n",
       "  'The',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'tokens',\n",
       "  'you',\n",
       "  'receive',\n",
       "  'during',\n",
       "  'the',\n",
       "  'Airdrop',\n",
       "  'will',\n",
       "  'depend',\n",
       "  'on',\n",
       "  'your',\n",
       "  'portfolio',\n",
       "  'and',\n",
       "  'testnet',\n",
       "  'status.',\n",
       "  'https://t.co/DiukBFV93K'],\n",
       " ['7/',\n",
       "  'What',\n",
       "  'to',\n",
       "  'do',\n",
       "  'next?',\n",
       "  '-Using',\n",
       "  'these',\n",
       "  'bridges:',\n",
       "  'üëâ',\n",
       "  'https://t.co/NwvBsrpz7F',\n",
       "  '(Polygon-Aptos)',\n",
       "  'üëâ',\n",
       "  'https://t.co/xZkt2LcElP',\n",
       "  '(ARB',\n",
       "  '-',\n",
       "  'AVAX)',\n",
       "  'https://t.co/DkipXr9jTH'],\n",
       " ['2/',\n",
       "  'There',\n",
       "  'is',\n",
       "  'information',\n",
       "  'that',\n",
       "  'cool',\n",
       "  'and',\n",
       "  'loaded',\n",
       "  'guys',\n",
       "  'brought',\n",
       "  'another',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'money',\n",
       "  '(~$120M',\n",
       "  'in',\n",
       "  'round',\n",
       "  'B)',\n",
       "  'from:',\n",
       "  'a16z,',\n",
       "  'sequioa,',\n",
       "  'Samsung',\n",
       "  'Next,',\n",
       "  'OpenSea',\n",
       "  'and',\n",
       "  'other',\n",
       "  'less',\n",
       "  'famous,',\n",
       "  'but',\n",
       "  'also',\n",
       "  'worthy',\n",
       "  'funds.',\n",
       "  'And',\n",
       "  'the',\n",
       "  'total',\n",
       "  'raise',\n",
       "  'already',\n",
       "  'amounts',\n",
       "  'to',\n",
       "  '~$293M.',\n",
       "  'üëâ',\n",
       "  'https://t.co/pV9m2jZkgO',\n",
       "  'https://t.co/N2gd2YDbTf'],\n",
       " ['So',\n",
       "  'cute',\n",
       "  '‚ù§Ô∏è',\n",
       "  'Credit:',\n",
       "  'Jukin',\n",
       "  'üî∏',\n",
       "  'For',\n",
       "  'more',\n",
       "  'inspiration',\n",
       "  'and',\n",
       "  'uplifting',\n",
       "  'stories,',\n",
       "  'please',\n",
       "  'follow',\n",
       "  'üëâ',\n",
       "  '@UpliftingVids',\n",
       "  'https://t.co/QLpkBTu38j'],\n",
       " ['A',\n",
       "  'new',\n",
       "  'model',\n",
       "  'with',\n",
       "  'his',\n",
       "  'best',\n",
       "  'day',\n",
       "  'so',\n",
       "  'far',\n",
       "  'on@Flirt4Free',\n",
       "  'on',\n",
       "  'april',\n",
       "  '7th:',\n",
       "  'Harper',\n",
       "  'Durand',\n",
       "  'üëâ',\n",
       "  'üëâ',\n",
       "  'https://t.co/3vR9Yj2DSR',\n",
       "  '@belamionline',\n",
       "  'Keep',\n",
       "  'up',\n",
       "  'the',\n",
       "  'good',\n",
       "  'work..',\n",
       "  'https://t.co/U8xcwa9K3g'],\n",
       " ['üèÖ',\n",
       "  '‚öõÔ∏è',\n",
       "  'ARC-Nucl√©art,',\n",
       "  'world-class',\n",
       "  'specialists',\n",
       "  'in',\n",
       "  'insecticide',\n",
       "  'and',\n",
       "  'fungicide',\n",
       "  'treatments',\n",
       "  'by',\n",
       "  'irradiation',\n",
       "  'and',\n",
       "  'restorations',\n",
       "  'using',\n",
       "  'radiation-curable',\n",
       "  'resins,',\n",
       "  'is',\n",
       "  'now',\n",
       "  'an',\n",
       "  '@iaeaorg',\n",
       "  'Collaborating',\n",
       "  'Centre.',\n",
       "  'üëâ',\n",
       "  'https://t.co/Vkfn1Jvspg',\n",
       "  'https://t.co/0QTMLMMxzD']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_tweets = all_tweets\n",
    "tweets = all_tweets #get_tweets(raw_tweets)\n",
    "token_tweets = []\n",
    "for tweet in tweets:\n",
    "  token_tweets.append(tweet_to_words_with_emojis(tweet))\n",
    "\n",
    "(token_tweets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = []\n",
    "# for tweet in tweets:\n",
    "#     for em in emoji.emoji_list(tweet):\n",
    "#         em = em[\"emoji\"]\n",
    "#         all_emojis.append(em)\n",
    "# all_emojis = list(set(all_emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_array = [tweet_to_words_with_emojis(tweet) for tweet in tweets]\n",
    "# tweets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = []\n",
    "# for tweet in tweets:\n",
    "#     for em in emoji.emoji_list(tweet):\n",
    "#         em = em[\"emoji\"]\n",
    "#         all_emojis.append(em)\n",
    "# all_emojis = list(set(all_emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emojis = []\n",
    "for tweet in token_tweets:\n",
    "    tokens = [token for token in tweet if emoji.is_emoji(token)]\n",
    "    all_emojis.extend(tokens)\n",
    "all_emojis = list(set(all_emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8qz1uPMkm51",
    "outputId": "4ee8d844-98d7-44c9-bb88-3d68b19a179c"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(token_tweets, min_count=1, max_vocab_size=None)\n",
    "model.build_vocab(token_tweets, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73685183, 82791480)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(token_tweets, total_examples=model.corpus_count, epochs=model.epochs, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.0239772e-01, -5.3765707e+00, -3.1128863e-01,  2.2337623e+00,\n",
       "       -1.5126020e+00,  3.8958380e+00, -3.4616716e+00,  9.3498915e-01,\n",
       "        4.6467695e+00, -2.3549285e+00, -9.0483183e-01,  4.2565164e-01,\n",
       "        4.2022276e+00, -2.7177212e+00, -6.4326340e-01,  6.0342914e-01,\n",
       "        7.7761449e-02, -5.4054016e-01,  2.8076386e-01, -3.0399007e-01,\n",
       "       -1.0196285e+00, -5.1881026e-02, -2.6516130e+00,  2.5474327e+00,\n",
       "        9.6406955e-01, -2.0042887e+00,  1.7409179e+00, -9.2644918e-01,\n",
       "        2.9051108e+00,  1.0117105e+00,  8.3809532e-02,  2.7588732e+00,\n",
       "        1.5294102e+00,  4.5669273e-01, -3.3542471e+00,  7.7384019e-01,\n",
       "       -2.2157575e-01, -3.4021541e-01,  3.5055488e-01, -4.5526454e-01,\n",
       "       -4.0888429e-01,  9.9432194e-01,  1.5269815e+00, -4.6535573e+00,\n",
       "        4.7304469e-01, -2.2350764e+00,  2.2937322e+00,  5.2155461e+00,\n",
       "        1.3824332e+00,  2.5452430e+00, -1.3880554e+00,  1.2415099e+00,\n",
       "        7.8836650e-01,  1.2881939e+00,  1.8563708e+00, -1.6374679e+00,\n",
       "       -1.0076431e+00, -4.4999585e+00,  1.0154425e+00,  1.9574269e+00,\n",
       "       -2.3771708e+00, -1.0755420e+00, -5.8587990e-04,  2.1843154e+00,\n",
       "       -1.1009789e+00, -7.6824374e-02, -2.3270048e-01, -2.8562567e+00,\n",
       "        3.2564437e+00, -5.5123073e-01,  1.4034461e+00,  2.9123962e-01,\n",
       "        4.0574293e+00, -2.9004798e+00, -6.3762712e-01, -1.4328146e+00,\n",
       "        9.5811874e-01, -1.9220071e+00,  2.5489137e-01,  1.2804651e+00,\n",
       "       -6.6654873e-01,  8.5568184e-01, -4.2913061e-01,  2.1615212e+00,\n",
       "       -1.9246778e+00, -1.8751585e+00,  1.6924071e+00, -4.2142129e+00,\n",
       "       -5.3347626e+00, -2.7993104e+00, -2.8287205e-01, -1.2410723e+00,\n",
       "        1.1993543e+00, -9.2056251e-01, -2.4617627e+00, -3.1838450e+00,\n",
       "       -1.9344455e+00, -1.8523401e+00, -5.3610701e-02, -1.8995478e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"‚ù§Ô∏è\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"emoji2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"emoji2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
